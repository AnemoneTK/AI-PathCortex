# backend/src/utils/vector_search.py
import os
import json
import numpy as np
import faiss
from typing import List, Dict, Any, Optional
from pathlib import Path
import sys
from colorama import init, Fore, Style

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô colorama
init(autoreset=True)

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô PYTHONPATH
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

try:
    from src.utils.logger import get_logger
    # ‡πÉ‡∏ä‡πâ logger ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß
    logger = get_logger("vector_search")
except ImportError:
    import logging
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logger ‡πÉ‡∏´‡∏°‡πà
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("vector_search")

class VectorSearch:
    def __init__(self, vector_db_dir: str, embedding_model=None):
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô VectorSearch
        
        Args:
            vector_db_dir: ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• vector
            embedding_model: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector)
        """
        # ‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ import SentenceTransformer
        try:
            from sentence_transformers import SentenceTransformer
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ embedding_model ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
            if embedding_model is None:
                default_model = 'intfloat/multilingual-e5-large'
                embedding_model = SentenceTransformer(default_model)
                print(f"{Fore.CYAN}üìö ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {default_model}{Style.RESET_ALL}")
        except ImportError:
            print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î SentenceTransformer ‡πÑ‡∏î‡πâ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector{Style.RESET_ALL}")
            embedding_model = None

        self.embedding_model = embedding_model
       
        print(f"{Fore.CYAN}‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô VectorSearch...{Style.RESET_ALL}")
        self.vector_db_dir = vector_db_dir
        self.embedding_model = embedding_model
        
        # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.job_knowledge_dir = os.path.join(vector_db_dir, "job_knowledge")
        self.career_advice_dir = os.path.join(vector_db_dir, "career_advice")
        
        # ‡πÑ‡∏ü‡∏•‡πå FAISS index ‡πÅ‡∏•‡∏∞ metadata
        self.job_index_file = os.path.join(self.job_knowledge_dir, "faiss_index.bin")
        self.job_metadata_file = os.path.join(self.job_knowledge_dir, "metadata.json")
        
        self.advice_index_file = os.path.join(self.career_advice_dir, "faiss_index.bin")
        self.advice_metadata_file = os.path.join(self.career_advice_dir, "metadata.json")
        
        print(f"{Fore.CYAN}üìÇ ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• vector: {vector_db_dir}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå job index: {self.job_index_file}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå job metadata: {self.job_metadata_file}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå advice index: {self.advice_index_file}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå advice metadata: {self.advice_metadata_file}{Style.RESET_ALL}")
        
        # ‡πÇ‡∏´‡∏•‡∏î metadata
        self.job_metadata = self._load_metadata(self.job_metadata_file)
        self.advice_metadata = self._load_metadata(self.advice_metadata_file)
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ
        self.processed_data_dir = os.path.join(project_root, "data", "processed")
        self.normalized_jobs_dir = os.path.join(self.processed_data_dir, "normalized_jobs")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping ‡∏Ç‡∏≠‡∏á job_id -> job_data
        self.jobs_data = self._load_jobs_data()
        
        if len(self.job_metadata) > 0 and len(self.advice_metadata) > 0:
            print(f"{Fore.GREEN}‚úÖ VectorSearch ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(self.job_metadata)} job metadata, {len(self.advice_metadata)} advice metadata{Style.RESET_ALL}")
        else:
            print(f"{Fore.YELLOW}‚ö†Ô∏è VectorSearch ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(self.job_metadata)} job metadata, {len(self.advice_metadata)} advice metadata{Style.RESET_ALL}")
        
        logger.info(f"VectorSearch ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(self.job_metadata)} job metadata, {len(self.advice_metadata)} advice metadata")
    
    def _load_metadata(self, metadata_file: str) -> List[Dict[str, Any]]:
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• metadata ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        try:
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå metadata: {metadata_file}")
                print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå metadata: {metadata_file}{Style.RESET_ALL}")
                return []
        except Exception as e:
            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î metadata: {str(e)}")
            print(f"{Fore.RED}‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î metadata: {str(e)}{Style.RESET_ALL}")
            return []
    
    def _load_jobs_data(self) -> Dict[str, Any]:
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        jobs_data = {}
        
        try:
            if os.path.exists(self.normalized_jobs_dir):
                for filename in os.listdir(self.normalized_jobs_dir):
                    if filename.endswith('.json'):
                        file_path = os.path.join(self.normalized_jobs_dir, filename)
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                job_data = json.load(f)
                                job_id = job_data.get('id')
                                if job_id:
                                    jobs_data[job_id] = job_data
                        except Exception as e:
                            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file_path}: {str(e)}")
                            print(f"{Fore.RED}‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file_path}: {str(e)}{Style.RESET_ALL}")
            
            print(f"{Fore.GREEN}‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(jobs_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£{Style.RESET_ALL}")
            logger.info(f"‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(jobs_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            return jobs_data
        except Exception as e:
            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û: {str(e)}")
            print(f"{Fore.RED}‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û: {str(e)}{Style.RESET_ALL}")
            return {}
    
    def get_job_by_id(self, job_id: str) -> Optional[Dict[str, Any]]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ï‡∏≤‡∏° ID"""
        return self.jobs_data.get(job_id)
    
    def search_jobs(self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            filters: ‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÄ‡∏ä‡πà‡∏ô {"skill": "python", "experience": "1-3"})
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        print(f"{Fore.CYAN}üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: \"{query}\"{Style.RESET_ALL}")
        logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: {query}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ index ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(self.job_index_file) or not os.path.exists(self.job_metadata_file):
            error_msg = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå FAISS index ‡∏´‡∏£‡∏∑‡∏≠ metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û"
            logger.error(error_msg)
            print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
            return []
        
        try:
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î FAISS index...{Style.RESET_ALL}")
            # ‡πÇ‡∏´‡∏•‡∏î FAISS index
            index = faiss.read_index(self.job_index_file)
            
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤...{Style.RESET_ALL}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            if self.embedding_model is None:
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
                print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô")
                query_embedding = np.random.random(1536).astype(np.float32)
                # Normalize vector
                query_embedding = query_embedding / np.linalg.norm(query_embedding)
            else:
                # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                query_embedding = self.embedding_model.encode([query])[0]
            
            print(f"{Fore.CYAN}üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô vector database...{Style.RESET_ALL}")
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô FAISS index
            query_embedding = np.array([query_embedding]).astype(np.float32)
            distances, indices = index.search(query_embedding, limit * 3)  # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ limit ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            results = []
            for i, idx in enumerate(indices[0]):
                if idx < len(self.job_metadata) and idx >= 0:  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï
                    job_id = self.job_metadata[idx]["id"]
                    job_data = self.get_job_by_id(job_id)
                    
                    if job_data:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö filters ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏
                        if filters and not self._match_filters(job_data, filters):
                            continue
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                        job_result = {
                            "id": job_id,
                            "title": job_data.get("titles", ["Unknown"])[0] if job_data.get("titles") else "Unknown",
                            "description": job_data.get("description", ""),
                            "responsibilities": job_data.get("responsibilities", []),
                            "skills": job_data.get("skills", []),
                            "salary_ranges": job_data.get("salary_ranges", []),
                            "education_requirements": job_data.get("education_requirements", []),
                            "similarity_score": float(1 / (1 + distances[0][i]))  # ‡πÅ‡∏õ‡∏•‡∏á distance ‡πÄ‡∏õ‡πá‡∏ô similarity score
                        }
                        results.append(job_result)
                        
                        # ‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏£‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡πâ‡∏ß
                        if len(results) >= limit:
                            break
            
            print(f"{Fore.GREEN}‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå{Style.RESET_ALL}")
            logger.info(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            if results:
                print(f"\n{Fore.CYAN}üîç ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:{Style.RESET_ALL}")
                for i, result in enumerate(results):
                    print(f"{i+1}. {Fore.GREEN}{result['title']}{Style.RESET_ALL} " + 
                         f"(‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô: {Fore.YELLOW}{result['similarity_score']:.2f}{Style.RESET_ALL})")
            else:
                print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ô‡∏µ‡πâ{Style.RESET_ALL}")
            
            return results
        
        except Exception as e:
            error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
            logger.error(error_msg)
            print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
            return []
    
    def _match_filters(self, job_data: Dict[str, Any], filters: Dict[str, Any]) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        for key, value in filters.items():
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏±‡∏Å‡∏©‡∏∞
            if key == "skill" and "skills" in job_data:
                if not any(value.lower() in skill.lower() for skill in job_data["skills"]):
                    return False
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå
            elif key == "experience" and "salary_ranges" in job_data:
                if not any(value == salary_range.get("experience") for salary_range in job_data["salary_ranges"]):
                    return False
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
            elif key == "education" and "education_requirements" in job_data:
                if not any(value.lower() in edu.lower() for edu in job_data["education_requirements"]):
                    return False
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
            elif key == "title" and "titles" in job_data:
                if not any(value.lower() in title.lower() for title in job_data["titles"]):
                    return False
        
        return True
    
    def search_career_advices(self, query: str, limit: int = 5, filter_tags: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            filter_tags: ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        print(f"{Fore.CYAN}üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: \"{query}\"{Style.RESET_ALL}")
        logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: {query}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filter_tags:
            print(f"{Fore.CYAN}üîñ ‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å: {', '.join(filter_tags)}{Style.RESET_ALL}")
            logger.info(f"‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å: {filter_tags}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ index ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(self.advice_index_file) or not os.path.exists(self.advice_metadata_file):
            error_msg = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå FAISS index ‡∏´‡∏£‡∏∑‡∏≠ metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û"
            logger.error(error_msg)
            print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
            return []
        
        try:
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î FAISS index...{Style.RESET_ALL}")
            # ‡πÇ‡∏´‡∏•‡∏î FAISS index
            index = faiss.read_index(self.advice_index_file)
            
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤...{Style.RESET_ALL}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            if self.embedding_model is None:
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
                print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô")
                query_embedding = np.random.random(1536).astype(np.float32)
                # Normalize vector
                query_embedding = query_embedding / np.linalg.norm(query_embedding)
            else:
                # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                query_embedding = self.embedding_model.encode([query])[0]
            
            print(f"{Fore.CYAN}üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô vector database...{Style.RESET_ALL}")
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô FAISS index
            query_embedding = np.array([query_embedding]).astype(np.float32)
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
            additional_results = 20 if filter_tags else 0
            distances, indices = index.search(query_embedding, limit + additional_results)
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            results = []
            filtered_count = 0
            
            for i, idx in enumerate(indices[0]):
                if idx < len(self.advice_metadata) and idx >= 0:  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï
                    item = self.advice_metadata[idx]
                    
                    # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏° tags ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏
                    if filter_tags and not any(tag in filter_tags for tag in item.get("tags", [])):
                        filtered_count += 1
                        continue
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    advice_result = {
                        "id": item.get("id", "unknown"),
                        "title": item.get("title", ""),
                        "text_preview": item.get("text", ""),
                        "tags": item.get("tags", []),
                        "source": item.get("source", ""),
                        "url": item.get("url", ""),
                        "similarity_score": float(1 / (1 + distances[0][i]))  # ‡πÅ‡∏õ‡∏•‡∏á distance ‡πÄ‡∏õ‡πá‡∏ô similarity score
                    }
                    results.append(advice_result)
                    
                    # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î
                    if len(results) >= limit:
                        break
            
            if filter_tags and filtered_count > 0:
                print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å {filtered_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î{Style.RESET_ALL}")
                logger.info(f"‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å {filtered_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î")
            
            print(f"{Fore.GREEN}‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á{Style.RESET_ALL}")
            logger.info(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            if results:
                print(f"\n{Fore.CYAN}üîç ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:{Style.RESET_ALL}")
                for i, result in enumerate(results):
                    tags_str = f", ‡πÅ‡∏ó‡πá‡∏Å: {', '.join(result['tags'])}" if result['tags'] else ""
                    print(f"{i+1}. {Fore.GREEN}{result['title']}{Style.RESET_ALL} " + 
                        f"(‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô: {Fore.YELLOW}{result['similarity_score']:.2f}{Style.RESET_ALL}{tags_str})")
            else:
                print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ô‡∏µ‡πâ{Style.RESET_ALL}")
            
            return results
        
        except Exception as e:
            error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
            logger.error(error_msg)
            print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
            return []
    
    def get_advice_document(self, advice_id: str) -> Optional[Dict[str, Any]]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ï‡∏≤‡∏° ID"""
        for item in self.advice_metadata:
            if item.get("id") == advice_id:
                return item
        return None