# backend/src/utils/vector_search.py
import os
import json
import numpy as np
import faiss
import re
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import sys
from colorama import init, Fore, Style
from difflib import get_close_matches

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô colorama
init(autoreset=True)

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô PYTHONPATH
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

try:
    from src.utils.logger import get_logger
    # ‡πÉ‡∏ä‡πâ logger ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß
    logger = get_logger("vector_search")
except ImportError:
    import logging
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logger ‡πÉ‡∏´‡∏°‡πà
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("vector_search")

class VectorSearch:
    def __init__(self, vector_db_dir: str, embedding_model=None):
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô VectorSearch
        
        Args:
            vector_db_dir: ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• vector
            embedding_model: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector)
        """
        # ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡πÑ‡∏≠‡∏ó‡∏µ
        self.tech_keywords = set([
            "programmer", "developer", "software", "web", "frontend", "backend", 
            "fullstack", "full stack", "data scientist", "data analyst", "devops", 
            "database", "engineer", "ux", "ui", "python", "java", "javascript", 
            "react", "angular", "node", "typescript", "c#", "c++", "php", "ruby", 
            "software engineer", "project manager", "scrum master", "product owner",
            "mobile", "android", "ios", "cloud", "aws", "azure", "network",
            "security", "system", "administrator", "qa", "testing"
        ])
        
        # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
        self.job_query_keywords = set([
            "‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô", "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà", "‡∏ó‡∏±‡∏Å‡∏©‡∏∞", "‡∏™‡∏Å‡∏¥‡∏•", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö", "‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤", 
            "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥", "‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå", "‡∏Ç‡πâ‡∏≠‡∏î‡∏µ", "‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢", "‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "‡∏™‡∏≤‡∏¢‡∏≠‡∏≤‡∏ä‡∏µ‡∏û",
            "salary", "responsibility", "skill", "education", "qualification", 
            "experience", "pros", "cons", "career path"
        ])
        
        # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö resume ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô
        self.resume_keywords = set([
            "resume", "‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà", "‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°", "cv", "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô", "‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå",
            "interview", "application", "portfolio", "‡∏û‡∏≠‡∏£‡πå‡∏ï‡πÇ‡∏ü‡∏•‡∏¥‡πÇ‡∏≠", "job hunt",
            "career", "‡∏≠‡∏≤‡∏ä‡∏µ‡∏û", "‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô", "writing", "template", "‡πÅ‡∏°‡πà‡πÅ‡∏ö‡∏ö", "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á"
        ])
        
        # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
        self.user_keywords = set([
            "user", "‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ", "profile", "‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå", "‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "student", 
            "‡∏à‡∏ö‡πÉ‡∏´‡∏°‡πà", "freshly graduated", "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß"
        ])
        
        # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        self.common_misspellings = {
            "badkend": "backend",
            "frentend": "frontend",
            "frontned": "frontend",
            "recat": "react",
            "javascrip": "javascript",
            "javascrpit": "javascript",
            "‡πÄ‡∏î‡πÄ‡∏ß‡∏•‡∏≠‡∏õ‡πÄ‡∏õ‡∏≠": "developer",
            "‡πÄ‡∏î‡πÄ‡∏ß‡∏•‡∏≠‡∏õ‡πÄ‡∏õ‡πâ‡∏≠": "developer",
            "‡πÄ‡∏î‡πÄ‡∏ß‡∏•‡∏≠‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå": "developer",
            "‡πÇ‡∏õ‡∏£‡πÄ‡πÄ‡∏Å‡∏£‡∏°‡πÄ‡∏°‡∏≠": "programmer",
            "‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏°‡∏≠": "programmer",
            "‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°": "resume",
            "‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà": "resume",
            "‡πÄ‡∏£‡∏ã‡∏π‡∏°‡πà": "resume",
            "‡πÄ‡∏£‡πâ‡∏ã‡∏π‡πÄ‡∏°‡πà": "resume"
        }
        
        try:
            from sentence_transformers import SentenceTransformer
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ embedding_model ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
            if embedding_model is None:
                default_model = 'intfloat/e5-small-v2'
                embedding_model = SentenceTransformer(default_model)
                print(f"{Fore.CYAN}üìö ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {default_model}{Style.RESET_ALL}")
        except ImportError:
            print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î SentenceTransformer ‡πÑ‡∏î‡πâ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector{Style.RESET_ALL}")
            embedding_model = None

        self.embedding_model = embedding_model

        print(f"{Fore.CYAN}‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô VectorSearch...{Style.RESET_ALL}")
        self.vector_db_dir = vector_db_dir
        self.embedding_model = embedding_model
        
        # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.job_knowledge_dir = os.path.join(vector_db_dir, "job_knowledge")
        self.career_advice_dir = os.path.join(vector_db_dir, "career_advice")
        self.combined_knowledge_dir = os.path.join(vector_db_dir, "combined_knowledge")
        
        # ‡πÑ‡∏ü‡∏•‡πå FAISS index ‡πÅ‡∏•‡∏∞ metadata
        self.job_index_file = os.path.join(self.job_knowledge_dir, "faiss_index.bin")
        self.job_metadata_file = os.path.join(self.job_knowledge_dir, "metadata.json")
        
        self.advice_index_file = os.path.join(self.career_advice_dir, "faiss_index.bin")
        self.advice_metadata_file = os.path.join(self.career_advice_dir, "metadata.json")
        
        self.combined_index_file = os.path.join(self.combined_knowledge_dir, "faiss_index.bin")
        self.combined_metadata_file = os.path.join(self.combined_knowledge_dir, "metadata.json")
        
        print(f"{Fore.CYAN}üìÇ ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• vector: {vector_db_dir}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå job index: {self.job_index_file}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå job metadata: {self.job_metadata_file}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå advice index: {self.advice_index_file}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå advice metadata: {self.advice_metadata_file}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå combined index: {self.combined_index_file}")
        print(f"{Fore.CYAN}üìÑ ‡πÑ‡∏ü‡∏•‡πå combined metadata: {self.combined_metadata_file}{Style.RESET_ALL}")
        
        # ‡πÇ‡∏´‡∏•‡∏î metadata
        self.job_metadata = self._load_metadata(self.job_metadata_file)
        self.advice_metadata = self._load_metadata(self.advice_metadata_file)
        self.combined_metadata = self._load_metadata(self.combined_metadata_file)
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ
        self.processed_data_dir = os.path.join(project_root, "data", "processed")
        try:
            # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å config ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            from src.utils.config import NORMALIZED_JOBS_DIR
            self.normalized_jobs_dir = NORMALIZED_JOBS_DIR
        except (ImportError, AttributeError):
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
            self.normalized_jobs_dir = os.path.join(self.processed_data_dir, "normalized_jobs")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping ‡∏Ç‡∏≠‡∏á job_id -> job_data
        self.jobs_data = self._load_jobs_data()
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å embedding_data.json ‡πÅ‡∏ó‡∏ô
        if len(self.job_metadata) == 0 and len(self.advice_metadata) == 0 and len(self.combined_metadata) == 0:
            print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• metadata ‡∏à‡∏∞‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å embedding_data.json ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
            self._load_fallback_metadata()
        
        if len(self.job_metadata) > 0 and len(self.advice_metadata) > 0:
            print(f"{Fore.GREEN}‚úÖ VectorSearch ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(self.job_metadata)} job metadata, {len(self.advice_metadata)} advice metadata{Style.RESET_ALL}")
            if len(self.combined_metadata) > 0:
                print(f"{Fore.GREEN}‚úÖ ‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°: {len(self.combined_metadata)} combined metadata{Style.RESET_ALL}")
        else:
            print(f"{Fore.YELLOW}‚ö†Ô∏è VectorSearch ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(self.job_metadata)} job metadata, {len(self.advice_metadata)} advice metadata{Style.RESET_ALL}")
        
        logger.info(f"VectorSearch ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(self.job_metadata)} job metadata, {len(self.advice_metadata)} advice metadata, {len(self.combined_metadata)} combined metadata")
    
    def _load_fallback_metadata(self):
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• metadata ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå fallback (embedding_data.json)"""
        try:
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå embedding_data.json ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå data/embedding
            embedding_data_path = os.path.join(project_root, "data", "embedding", "embedding_data.json")
            advice_data_path = os.path.join(project_root, "data", "embedding", "career_advices_embeddings.json")
            
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
            if os.path.exists(embedding_data_path):
                print(f"{Fore.CYAN}üìÑ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û: {embedding_data_path}{Style.RESET_ALL}")
                with open(embedding_data_path, 'r', encoding='utf-8') as f:
                    fallback_data = json.load(f)
                    self.job_metadata = fallback_data
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô jobs_data
                    for job in fallback_data:
                        job_id = job.get("id")
                        if job_id:
                            self.jobs_data[job_id] = job.get("metadata", {})
                
                print(f"{Fore.GREEN}‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏à‡∏≤‡∏Å fallback ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(self.job_metadata)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£{Style.RESET_ALL}")
            
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
            if os.path.exists(advice_data_path):
                print(f"{Fore.CYAN}üìÑ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {advice_data_path}{Style.RESET_ALL}")
                with open(advice_data_path, 'r', encoding='utf-8') as f:
                    self.advice_metadata = json.load(f)
                
                print(f"{Fore.GREEN}‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å fallback ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(self.advice_metadata)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£{Style.RESET_ALL}")
            
        except Exception as e:
            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• fallback: {str(e)}")
            print(f"{Fore.RED}‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• fallback: {str(e)}{Style.RESET_ALL}")
    
    def _load_metadata(self, metadata_file: str) -> List[Dict[str, Any]]:
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• metadata ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        try:
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                    if isinstance(data, dict) and "job_data" in data:
                        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á JobDataNormalizer
                        return data.get("job_data", [])
                    elif isinstance(data, dict) and "advice_data" in data:
                        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á JobDataNormalizer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
                        return data.get("advice_data", [])
                    else:
                        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô list
                        return data
            else:
                logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå metadata: {metadata_file}")
                print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå metadata: {metadata_file}{Style.RESET_ALL}")
                return []
        except Exception as e:
            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î metadata: {str(e)}")
            print(f"{Fore.RED}‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î metadata: {str(e)}{Style.RESET_ALL}")
            return []
    
    def _load_jobs_data(self) -> Dict[str, Any]:
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        jobs_data = {}
        
        try:
            if os.path.exists(self.normalized_jobs_dir):
                for filename in os.listdir(self.normalized_jobs_dir):
                    if filename.endswith('.json'):
                        file_path = os.path.join(self.normalized_jobs_dir, filename)
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                job_data = json.load(f)
                                job_id = job_data.get('id')
                                if job_id:
                                    jobs_data[job_id] = job_data
                        except Exception as e:
                            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file_path}: {str(e)}")
                            print(f"{Fore.RED}‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file_path}: {str(e)}{Style.RESET_ALL}")
            
            print(f"{Fore.GREEN}‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(jobs_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£{Style.RESET_ALL}")
            logger.info(f"‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(jobs_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            return jobs_data
        except Exception as e:
            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û: {str(e)}")
            print(f"{Fore.RED}‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û: {str(e)}{Style.RESET_ALL}")
            return {}
    
    def _normalize_query(self, query: str) -> Tuple[str, List[str]]:
        """
        ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            
        Returns:
            Tuple[str, List[str]]: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        """
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î
        words = query.lower().split()
        corrected_words = []
        
        for word in words:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
            if word in self.common_misspellings:
                corrected_words.append(self.common_misspellings[word])
            else:
                corrected_words.append(word)
        
        corrected_query = " ".join(corrected_words)
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        keywords = []
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
        for tech_term in self.tech_keywords:
            if tech_term.lower() in corrected_query.lower():
                keywords.append(tech_term)
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        for query_term in self.job_query_keywords:
            if query_term.lower() in corrected_query.lower():
                keywords.append(query_term)
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        if not keywords:
            # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏≠‡∏Å
            query_without_question = re.sub(r'(‡∏¢‡∏±‡∏á‡πÑ‡∏á|‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£|‡∏°‡∏±‡πâ‡∏¢|‡πÑ‡∏´‡∏°|‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£|‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà|\?)+\s*$', '', corrected_query)
            keywords = query_without_question.split()
        
        return corrected_query, keywords
    
    def get_job_by_id(self, job_id: str) -> Optional[Dict[str, Any]]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ï‡∏≤‡∏° ID"""
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• fallback
        job_data = self.jobs_data.get(job_id)
        if not job_data:
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å job_metadata ‡∏î‡πâ‡∏ß‡∏¢
            for job in self.job_metadata:
                if job.get("id") == job_id:
                    return job.get("metadata", {})
        return job_data
    
    def _create_mock_embedding(self, text: str, dimension: int = 384) -> np.ndarray:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ model
        """
        # ‡πÉ‡∏ä‡πâ hash ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        text_hash = hash(text) % 2**32
        np.random.seed(text_hash)
        vector = np.random.random(dimension).astype(np.float32)
        # Normalize vector
        vector = vector / np.linalg.norm(vector)
        return vector
    
    def _fallback_search(self, query: str, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ FAISS index
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
            keywords: ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            
        Returns:
            List[Dict[str, Any]]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        results = []
        
        for job in self.job_metadata:
            score = 0
            job_id = job.get("id", "")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if job_id:
                for keyword in keywords:
                    if keyword.lower() in job_id.lower():
                        score += 5
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            job_text = job.get("text", "")
            if job_text:
                for keyword in keywords:
                    if keyword.lower() in job_text.lower():
                        score += 1
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            job_titles = job.get("metadata", {}).get("titles", [])
            if job_titles:
                for title in job_titles:
                    for keyword in keywords:
                        if keyword.lower() in title.lower():
                            score += 3
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            job_skills = job.get("metadata", {}).get("skills", [])
            if job_skills:
                for skill in job_skills:
                    for keyword in keywords:
                        if keyword.lower() in skill.lower():
                            score += 2
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0
            if score > 0:
                job_result = {
                    "id": job_id,
                    "title": job_titles[0] if job_titles else job_id,
                    "description": job_text,
                    "responsibilities": job.get("metadata", {}).get("responsibilities", []),
                    "skills": job_skills,
                    "salary_ranges": job.get("metadata", {}).get("salary_ranges", []),
                    "education_requirements": job.get("metadata", {}).get("education_requirements", []),
                    "similarity_score": float(score / 10)  # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏õ‡πá‡∏ô similarity score
                }
                results.append(job_result)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        return results[:limit]
    
    def _fallback_search_advices(self, query: str, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ FAISS index
        """
        results = []
        
        for advice in self.advice_metadata:
            score = 0
            advice_id = advice.get("id", "")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if advice_id:
                for keyword in keywords:
                    if keyword.lower() in advice_id.lower():
                        score += 5
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            advice_text = advice.get("text", "")
            if advice_text:
                for keyword in keywords:
                    if keyword.lower() in advice_text.lower():
                        score += 1
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            advice_title = advice.get("title", "")
            if advice_title:
                for keyword in keywords:
                    if keyword.lower() in advice_title.lower():
                        score += 3
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            advice_tags = advice.get("tags", [])
            if advice_tags:
                for tag in advice_tags:
                    for keyword in keywords:
                        if keyword.lower() in tag.lower():
                            score += 2
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0
            if score > 0:
                advice_result = {
                    "id": advice_id,
                    "title": advice_title,
                    "text_preview": advice_text,
                    "tags": advice_tags,
                    "source": advice.get("source", ""),
                    "url": advice.get("url", ""),
                    "similarity_score": float(score / 10)  # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏õ‡πá‡∏ô similarity score
                }
                results.append(advice_result)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        return results[:limit]
    
    def search_jobs(self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            filters: ‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÄ‡∏ä‡πà‡∏ô {"skill": "python", "experience": "1-3"})
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        print(f"{Fore.CYAN}üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: \"{query}\"{Style.RESET_ALL}")
        logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: {query}")
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        corrected_query, keywords = self._normalize_query(query)
        
        if corrected_query != query:
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: \"{corrected_query}\"{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(keywords)}{Style.RESET_ALL}")
            logger.info(f"‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: \"{corrected_query}\", ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(keywords)}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ index ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(self.job_index_file) or not os.path.exists(self.job_metadata_file):
            warning_msg = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå FAISS index ‡∏´‡∏£‡∏∑‡∏≠ metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô"
            logger.warning(warning_msg)
            print(f"{Fore.YELLOW}‚ö†Ô∏è {warning_msg}{Style.RESET_ALL}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ FAISS index ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
            return self._fallback_search(corrected_query, keywords, limit)
        
        try:
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î FAISS index...{Style.RESET_ALL}")
            # ‡πÇ‡∏´‡∏•‡∏î FAISS index
            index = faiss.read_index(self.job_index_file)
            
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤...{Style.RESET_ALL}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            try:
                if self.embedding_model is None:
                    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
                    print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                    logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô")
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°
                    query_embedding = self._create_mock_embedding(" ".join(keywords), dimension=index.d)
                else:
                    # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                    query_embedding = self.embedding_model.encode([corrected_query])[0]
                    # Normalize vector
                    query_embedding = query_embedding / np.linalg.norm(query_embedding)
                
                print(f"{Fore.CYAN}üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô vector database...{Style.RESET_ALL}")
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô FAISS index
                query_embedding = np.array([query_embedding]).astype(np.float32)
                distances, indices = index.search(query_embedding, limit * 3)  # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ limit ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                results = []
                for i, idx in enumerate(indices[0]):
                    if idx < 0 or idx >= len(self.job_metadata):
                        continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                        
                    job_id = self.job_metadata[idx]["id"]
                    job_data = self.get_job_by_id(job_id)
                    
                    if job_data:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö filters ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏
                        if filters and not self._match_filters(job_data, filters):
                            continue
                        
                        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
                        job_title = "Unknown"
                        if "titles" in job_data and job_data["titles"]:
                            job_title = job_data["titles"][0]
                        elif "title" in job_data:
                            job_title = job_data["title"]
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                        job_result = {
                            "id": job_id,
                            "title": job_title,
                            "description": job_data.get("description", ""),
                            "responsibilities": job_data.get("responsibilities", []),
                            "skills": job_data.get("skills", []),
                            "salary_ranges": job_data.get("salary_ranges", []),
                            "education_requirements": job_data.get("education_requirements", []),
                            "similarity_score": float(1 / (1 + distances[0][i]))  # ‡πÅ‡∏õ‡∏•‡∏á distance ‡πÄ‡∏õ‡πá‡∏ô similarity score
                        }
                        results.append(job_result)
                        
                        # ‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏£‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡πâ‡∏ß
                        if len(results) >= limit:
                            break
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÄ‡∏™‡∏£‡∏¥‡∏°
                if len(results) < limit:
                    fallback_results = self._fallback_search(corrected_query, keywords, limit - len(results))
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å fallback ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
                    existing_ids = {result["id"] for result in results}
                    for result in fallback_results:
                        if result["id"] not in existing_ids:
                            results.append(result)
                            if len(results) >= limit:
                                break
                
                print(f"{Fore.GREEN}‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå{Style.RESET_ALL}")
                logger.info(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° similarity_score
                results.sort(key=lambda x: x["similarity_score"], reverse=True)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                if results:
                    print(f"\n{Fore.CYAN}üîç ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:{Style.RESET_ALL}")
                    for i, result in enumerate(results):
                        print(f"{i+1}. {Fore.GREEN}{result['title']}{Style.RESET_ALL} " + 
                            f"(‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô: {Fore.YELLOW}{result['similarity_score']:.2f}{Style.RESET_ALL})")
                else:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ô‡∏µ‡πâ{Style.RESET_ALL}")
                
                return results
                
            except Exception as e:
                error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
                logger.error(error_msg)
                print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
                
                # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
                print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                return self._fallback_search(corrected_query, keywords, limit)
                
        except Exception as e:
            error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
            logger.error(error_msg)
            print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
            
            # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
            return self._fallback_search(corrected_query, keywords, limit)
    
    def _match_filters(self, job_data: Dict[str, Any], filters: Dict[str, Any]) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        for key, value in filters.items():
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏±‡∏Å‡∏©‡∏∞
            if key == "skill" and "skills" in job_data:
                if not any(value.lower() in skill.lower() for skill in job_data["skills"]):
                    return False
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå
            elif key == "experience" and "salary_ranges" in job_data:
                if not any(value == salary_range.get("experience") for salary_range in job_data["salary_ranges"]):
                    return False
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
            elif key == "education" and "education_requirements" in job_data:
                if not any(value.lower() in edu.lower() for edu in job_data["education_requirements"]):
                    return False
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
            elif key == "title" and "titles" in job_data:
                if not any(value.lower() in title.lower() for title in job_data["titles"]):
                    return False
        
        return True
    
    def search_career_advices(self, query: str, limit: int = 5, filter_tags: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            filter_tags: ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        print(f"{Fore.CYAN}üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: \"{query}\"{Style.RESET_ALL}")
        logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: {query}")
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        corrected_query, keywords = self._normalize_query(query)
        
        if corrected_query != query:
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: \"{corrected_query}\"{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(keywords)}{Style.RESET_ALL}")
            logger.info(f"‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: \"{corrected_query}\", ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(keywords)}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filter_tags:
            print(f"{Fore.CYAN}üîñ ‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å: {', '.join(filter_tags)}{Style.RESET_ALL}")
            logger.info(f"‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å: {filter_tags}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ index ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(self.advice_index_file) or not os.path.exists(self.advice_metadata_file):
            warning_msg = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå FAISS index ‡∏´‡∏£‡∏∑‡∏≠ metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô"
            logger.warning(warning_msg)
            print(f"{Fore.YELLOW}‚ö†Ô∏è {warning_msg}{Style.RESET_ALL}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ FAISS index ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
            return self._fallback_search_advices(corrected_query, keywords, limit)
        
        try:
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î FAISS index...{Style.RESET_ALL}")
            # ‡πÇ‡∏´‡∏•‡∏î FAISS index
            index = faiss.read_index(self.advice_index_file)
            
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤...{Style.RESET_ALL}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            try:
                if self.embedding_model is None:
                    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
                    print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                    logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô")
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°
                    query_embedding = self._create_mock_embedding(" ".join(keywords), dimension=index.d)
                else:
                    # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                    query_embedding = self.embedding_model.encode([corrected_query])[0]
                    # Normalize vector
                    query_embedding = query_embedding / np.linalg.norm(query_embedding)
                
                print(f"{Fore.CYAN}üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô vector database...{Style.RESET_ALL}")
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô FAISS index
                query_embedding = np.array([query_embedding]).astype(np.float32)
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
                additional_results = 20 if filter_tags else 0
                distances, indices = index.search(query_embedding, limit + additional_results)
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                results = []
                filtered_count = 0
                
                for i, idx in enumerate(indices[0]):
                    if idx < 0 or idx >= len(self.advice_metadata):
                        continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                        
                    item = self.advice_metadata[idx]
                    
                    # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏° tags ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏
                    if filter_tags:
                        item_tags = item.get("tags", [])
                        if not any(tag in filter_tags for tag in item_tags):
                            filtered_count += 1
                            continue
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    advice_result = {
                        "id": item.get("id", "unknown"),
                        "title": item.get("title", ""),
                        "text_preview": item.get("text", ""),
                        "tags": item.get("tags", []),
                        "source": item.get("source", ""),
                        "url": item.get("url", ""),
                        "similarity_score": float(1 / (1 + distances[0][i]))  # ‡πÅ‡∏õ‡∏•‡∏á distance ‡πÄ‡∏õ‡πá‡∏ô similarity score
                    }
                    results.append(advice_result)
                    
                    # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î
                    if len(results) >= limit:
                        break
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÄ‡∏™‡∏£‡∏¥‡∏°
                if len(results) < limit:
                    fallback_results = self._fallback_search_advices(corrected_query, keywords, limit - len(results))
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å fallback ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
                    existing_ids = {result["id"] for result in results}
                    for result in fallback_results:
                        if result["id"] not in existing_ids:
                            results.append(result)
                            if len(results) >= limit:
                                break
                
                if filter_tags and filtered_count > 0:
                    print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å {filtered_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î{Style.RESET_ALL}")
                    logger.info(f"‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å {filtered_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î")
                
                print(f"{Fore.GREEN}‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á{Style.RESET_ALL}")
                logger.info(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° similarity_score
                results.sort(key=lambda x: x["similarity_score"], reverse=True)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                if results:
                    print(f"\n{Fore.CYAN}üîç ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:{Style.RESET_ALL}")
                    for i, result in enumerate(results):
                        tags_str = f", ‡πÅ‡∏ó‡πá‡∏Å: {', '.join(result['tags'])}" if result['tags'] else ""
                        print(f"{i+1}. {Fore.GREEN}{result['title']}{Style.RESET_ALL} " + 
                            f"(‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô: {Fore.YELLOW}{result['similarity_score']:.2f}{Style.RESET_ALL}{tags_str})")
                else:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ô‡∏µ‡πâ{Style.RESET_ALL}")
                
                return results
                
            except Exception as e:
                error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
                logger.error(error_msg)
                print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
                
                # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
                print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                return self._fallback_search_advices(corrected_query, keywords, limit)
            
        except Exception as e:
            error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
            logger.error(error_msg)
            print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
            
            # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
            return self._fallback_search_advices(corrected_query, keywords, limit)
    
    def get_advice_document(self, advice_id: str) -> Optional[Dict[str, Any]]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ï‡∏≤‡∏° ID"""
        for item in self.advice_metadata:
            if item.get("id") == advice_id:
                return item
        return None
    
    def search_relevant_advices(self, query: str, limit: int = 5, filter_tags: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            filter_tags: ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        print(f"{Fore.CYAN}üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: \"{query}\"{Style.RESET_ALL}")
        logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö: {query}")
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        corrected_query, keywords = self._normalize_query(query)
        
        if corrected_query != query:
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: \"{corrected_query}\"{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(keywords)}{Style.RESET_ALL}")
            logger.info(f"‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: \"{corrected_query}\", ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(keywords)}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filter_tags:
            print(f"{Fore.CYAN}üîñ ‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å: {', '.join(filter_tags)}{Style.RESET_ALL}")
            logger.info(f"‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ó‡πá‡∏Å: {filter_tags}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ index ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(self.advice_index_file) or not os.path.exists(self.advice_metadata_file):
            warning_msg = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå FAISS index ‡∏´‡∏£‡∏∑‡∏≠ metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô"
            logger.warning(warning_msg)
            print(f"{Fore.YELLOW}‚ö†Ô∏è {warning_msg}{Style.RESET_ALL}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ FAISS index ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
            return self._fallback_search_advices(corrected_query, keywords, limit)
        
        try:
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î FAISS index...{Style.RESET_ALL}")
            # ‡πÇ‡∏´‡∏•‡∏î FAISS index
            index = faiss.read_index(str(self.advice_index_file))
            
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤...{Style.RESET_ALL}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            try:
                if self.embedding_model is None:
                    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
                    print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                    logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô")
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°
                    query_embedding = self._create_mock_embedding(" ".join(keywords), dimension=index.d)
                else:
                    # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                    query_embedding = self.embedding_model.encode([corrected_query])[0]
                    # Normalize vector
                    query_embedding = query_embedding / np.linalg.norm(query_embedding)
                
                print(f"{Fore.CYAN}üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô vector database...{Style.RESET_ALL}")
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô FAISS index
                query_embedding = np.array([query_embedding]).astype(np.float32)
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
                additional_results = 20 if filter_tags else 0
                distances, indices = index.search(query_embedding, limit + additional_results)
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                results = []
                filtered_count = 0
                
                for i, idx in enumerate(indices[0]):
                    if idx < 0 or idx >= len(self.advice_metadata):
                        continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                        
                    item = self.advice_metadata[idx]
                    
                    # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏° tags ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏
                    if filter_tags:
                        item_tags = item.get("tags", [])
                        if not any(tag in filter_tags for tag in item_tags):
                            filtered_count += 1
                            continue
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    advice_result = {
                        "id": item.get("id", "unknown"),
                        "title": item.get("title", ""),
                        "text_preview": item.get("text", ""),
                        "tags": item.get("tags", []),
                        "source": item.get("source", ""),
                        "url": item.get("url", ""),
                        "similarity_score": float(1 / (1 + distances[0][i]))  # ‡πÅ‡∏õ‡∏•‡∏á distance ‡πÄ‡∏õ‡πá‡∏ô similarity score
                    }
                    results.append(advice_result)
                    
                    # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î
                    if len(results) >= limit:
                        break
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÄ‡∏™‡∏£‡∏¥‡∏°
                if len(results) < limit:
                    fallback_results = self._fallback_search_advices(corrected_query, keywords, limit - len(results))
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å fallback ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
                    existing_ids = {result["id"] for result in results}
                    for result in fallback_results:
                        if result["id"] not in existing_ids:
                            results.append(result)
                            if len(results) >= limit:
                                break
                
                if filter_tags and filtered_count > 0:
                    print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å {filtered_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î{Style.RESET_ALL}")
                    logger.info(f"‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å {filtered_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î")
                
                print(f"{Fore.GREEN}‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á{Style.RESET_ALL}")
                logger.info(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° similarity_score
                results.sort(key=lambda x: x["similarity_score"], reverse=True)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                if results:
                    print(f"\n{Fore.CYAN}üîç ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:{Style.RESET_ALL}")
                    for i, result in enumerate(results):
                        tags_str = f", ‡πÅ‡∏ó‡πá‡∏Å: {', '.join(result['tags'])}" if result['tags'] else ""
                        print(f"{i+1}. {Fore.GREEN}{result['title']}{Style.RESET_ALL} " + 
                            f"(‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô: {Fore.YELLOW}{result['similarity_score']:.2f}{Style.RESET_ALL}{tags_str})")
                else:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ô‡∏µ‡πâ{Style.RESET_ALL}")
                
                return results
                
            except Exception as e:
                error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
                logger.error(error_msg)
                print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
                
                # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
                print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                return self._fallback_search_advices(corrected_query, keywords, limit)
            
        except Exception as e:
            error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
            logger.error(error_msg)
            print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
            
            # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö fallback ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
            return self._fallback_search_advices(corrected_query, keywords, limit)
    
    def search_combined(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        print(f"{Fore.CYAN}üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: \"{query}\"{Style.RESET_ALL}")
        logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: {query}")
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        corrected_query, keywords = self._normalize_query(query)
        
        # ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
        query_type = self._identify_query_type(query, keywords)
        
        if corrected_query != query:
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: \"{corrected_query}\"{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(keywords)}{Style.RESET_ALL}")
            logger.info(f"‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: \"{corrected_query}\", ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö: {', '.join(keywords)}")
        
        print(f"{Fore.CYAN}üîç ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query_type}{Style.RESET_ALL}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ index ‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏°‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(self.combined_index_file) or not os.path.exists(self.combined_metadata_file):
            warning_msg = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå FAISS index ‡∏´‡∏£‡∏∑‡∏≠ metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏° ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÅ‡∏ó‡∏ô"
            logger.warning(warning_msg)
            print(f"{Fore.YELLOW}‚ö†Ô∏è {warning_msg}{Style.RESET_ALL}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
            if query_type == "user":
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
                users = self._fallback_search_users(corrected_query, keywords, limit)
                return users
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö resume ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô
            elif query_type == "resume":
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
                advices = self.search_career_advices(query, limit)
                return advices
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
            else:
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
                jobs = self.search_jobs(query, limit)
                return jobs
        
        try:
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î FAISS index...{Style.RESET_ALL}")
            # ‡πÇ‡∏´‡∏•‡∏î FAISS index ‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏°
            index = faiss.read_index(self.combined_index_file)
            
            print(f"{Fore.CYAN}‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤...{Style.RESET_ALL}")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            try:
                if self.embedding_model is None:
                    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
                    print(f"{Fore.YELLOW}‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô{Style.RESET_ALL}")
                    logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á vector ‡πÅ‡∏ó‡∏ô")
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°
                    query_embedding = self._create_mock_embedding(" ".join(keywords), dimension=index.d)
                else:
                    # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                    query_embedding = self.embedding_model.encode([corrected_query])[0]
                    # Normalize vector
                    query_embedding = query_embedding / np.linalg.norm(query_embedding)
                
                print(f"{Fore.CYAN}üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô vector database...{Style.RESET_ALL}")
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô FAISS index
                query_embedding = np.array([query_embedding]).astype(np.float32)
                distances, indices = index.search(query_embedding, limit * 2)  # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                results = []
                
                # ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
                type_weights = {
                    "job": 1.0 if query_type == "job" else 0.6,
                    "advice": 1.0 if query_type == "resume" else 0.7,
                    "user": 1.0 if query_type == "user" else 0.5
                }
                
                # ‡πÇ‡∏´‡∏•‡∏î metadata ‡∏à‡∏≤‡∏Å combined_metadata
                item_types = self.combined_metadata.get("item_types", [])
                item_data = self.combined_metadata.get("item_data", [])
                
                processed_results = []
                
                for i, idx in enumerate(indices[0]):
                    if idx < 0 or idx >= len(item_data):
                        continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                    
                    item_type = item_types[idx] if idx < len(item_types) else "unknown"
                    item = item_data[idx]
                    
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                    similarity_score = 1.0 / (1.0 + distances[0][i])
                    weighted_score = similarity_score * type_weights.get(item_type, 0.5)
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    result = {
                        "id": item.get("id", ""),
                        "type": item_type,
                        "title": item.get("title", ""),
                        "similarity_score": float(similarity_score),
                        "weighted_score": float(weighted_score),
                        "content": {}
                    }
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                    if item_type == "job":
                        result["content"] = {
                            "description": item.get("description", ""),
                            "responsibilities": item.get("responsibilities", []),
                            "skills": item.get("skills", []),
                            "salary_ranges": item.get("salary_ranges", [])
                        }
                    elif item_type == "advice":
                        result["content"] = {
                            "text_preview": item.get("text_preview", ""),
                            "tags": item.get("tags", []),
                            "source": item.get("source", ""),
                            "url": item.get("url", "")
                        }
                    elif item_type == "user":
                        result["content"] = {
                            "name": item.get("name", ""),
                            "institution": item.get("institution", ""),
                            "education_status": item.get("education_status", ""),
                            "skills": item.get("skills", [])
                        }
                    
                    processed_results.append(result)
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° weighted_score
                processed_results.sort(key=lambda x: x["weighted_score"], reverse=True)
                
                # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                results = processed_results[:limit]
                
                print(f"{Fore.GREEN}‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå{Style.RESET_ALL}")
                logger.info(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏û‡∏ö {len(results)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                if results:
                    print(f"\n{Fore.CYAN}üîç ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:{Style.RESET_ALL}")
                    for i, result in enumerate(results):
                        item_type = result["type"]
                        item_type_str = {
                            "job": "‡∏≠‡∏≤‡∏ä‡∏µ‡∏û",
                            "advice": "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
                            "user": "‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ"
                        }.get(item_type, item_type)
                        
                        print(f"{i+1}. {Fore.GREEN}{result['title']}{Style.RESET_ALL} " + 
                            f"({item_type_str}, ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {Fore.YELLOW}{result['weighted_score']:.2f}{Style.RESET_ALL})")
                else:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ô‡∏µ‡πâ{Style.RESET_ALL}")
                
                return results
                    
            except Exception as e:
                error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {str(e)}"
                logger.error(error_msg)
                print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
                
                # ‡∏ó‡∏≥ fallback ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
                if query_type == "resume":
                    return self.search_career_advices(query, limit)
                elif query_type == "user":
                    return self._fallback_search_users(corrected_query, keywords, limit)
                else:
                    return self.search_jobs(query, limit)
        
        except Exception as e:
            error_msg = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏°: {str(e)}"
            logger.error(error_msg)
            print(f"{Fore.RED}‚ùå {error_msg}{Style.RESET_ALL}")
            
            # ‡∏ó‡∏≥ fallback ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            if query_type == "resume":
                return self.search_career_advices(query, limit)
            elif query_type == "user":
                return self._fallback_search_users(corrected_query, keywords, limit)
            else:
                return self.search_jobs(query, limit)
                    
    def _identify_query_type(self, query: str, keywords: List[str]) -> str:
        """
        ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            keywords: ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            
        Returns:
            str: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ("job", "resume", "user")
        """
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
        job_count = sum(1 for kw in keywords if kw.lower() in self.tech_keywords or kw.lower() in self.job_query_keywords)
        resume_count = sum(1 for kw in keywords if kw.lower() in self.resume_keywords)
        user_count = sum(1 for kw in keywords if kw.lower() in self.user_keywords)
        
        # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        if user_count > 0 and (user_count >= job_count and user_count >= resume_count):
            return "user"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö resume
        if resume_count > 0 and (resume_count >= job_count):
            # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö resume ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô
            return "resume"
        
        # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏ä‡∏µ‡∏û (‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)
        return "job"

    def _fallback_search_users(self, query: str, keywords: List[str], limit: int = 5) -> List[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö fallback ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ FAISS index
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
            keywords: ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            
        Returns:
            List[Dict[str, Any]]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        results = []
        
        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
            from src.utils.config import USERS_DIR
            users_file = os.path.join(USERS_DIR, "users.json")
            
            if not os.path.exists(users_file):
                logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {users_file}")
                return []
            
            with open(users_file, 'r', encoding='utf-8') as f:
                users_data = json.load(f)
            
            for user in users_data:
                score = 0
                user_id = user.get("id", "")
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠
                if "name" in user and any(kw.lower() in user["name"].lower() for kw in keywords):
                    score += 5
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
                if "institution" in user and any(kw.lower() in user["institution"].lower() for kw in keywords):
                    score += 3
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡∏Å‡∏©‡∏∞
                if "skills" in user:
                    for skill in user["skills"]:
                        skill_name = skill.get("name", "")
                        if any(kw.lower() in skill_name.lower() for kw in keywords):
                            score += 2
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°
                if "programming_languages" in user:
                    for lang in user["programming_languages"]:
                        if any(kw.lower() in lang.lower() for kw in keywords):
                            score += 2
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0
                if score > 0:
                    user_result = {
                        "id": f"user_{user_id}",
                        "type": "user",
                        "title": user.get("name", f"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ {user_id}"),
                        "similarity_score": float(score / 10),
                        "weighted_score": float(score / 10),
                        "content": {
                            "name": user.get("name", ""),
                            "institution": user.get("institution", ""),
                            "education_status": user.get("education_status", ""),
                            "skills": [skill.get("name") for skill in user.get("skills", [])]
                        }
                    }
                    results.append(user_result)
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
            results.sort(key=lambda x: x["weighted_score"], reverse=True)
            
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            return results[:limit]
            
        except Exception as e:
            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {str(e)}")
            return []
    
    